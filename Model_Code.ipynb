{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBNnhTUVKzZi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1_4ubiC_reM"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-jCMKIRlAe"
      },
      "outputs": [],
      "source": [
        "# SE model has dense layers 512-64-2\n",
        "from SE_Xception_GAP  import *\n",
        "# from ELBP_Functions     import *\n",
        "from ELBP_Custom_Layers import *\n",
        "\n",
        "from Parameter_Calc import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ31kW_0K3rQ"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm.notebook import tqdm as log_progress\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import data_utils\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "import os\n",
        "import pandas  as pd\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "#from keras.engine.topology import get_source_inputs\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras import backend as K\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.vis_utils import plot_model\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +    'haarcascade_frontalface_default.xml')\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers import BatchNormalization\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper Funciton to get Model Photo\n",
        "def model_photu(model,path='model_plot.png',n=True,expand=False,direct='TB',shapes=True):\n",
        "   return plot_model(model, to_file=os.getcwd()+path, show_shapes=shapes, show_layer_names=n,expand_nested=expand,rankdir=direct)"
      ],
      "metadata": {
        "id": "GYeMAs_hNDRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vXcapen5wSh"
      },
      "outputs": [],
      "source": [
        "# Dataset imports and Extraction\n",
        "\n",
        "!7z x '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/replay_data_every_10th.zip' -o'/content/'\n",
        "!7z x '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/CASIA_FASD_FRAMES.zip' -o'/content/'\n",
        "!7z x '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/MSU-MFSD_FRAMES.zip' -o'/content/MSU-MFSD_FRAMES/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCCEB6CiZ818"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Folders\n",
        "Replay_train_folder = '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/replay_data_every_10th/train'\n",
        "Replay_val_folder   = '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/replay_data_every_10th/dev'\n",
        "Casia_train_folder  = '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/CASIA_FASD_FRAMES/train_release'\n",
        "Casia_val_folder    = Casia_train_folder\n",
        "Oulu_train_folder   = '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/OULU_frames/Train_Files'\n",
        "Oulu_val_folder     = '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/OULU_frames/Dev_Files_frames'\n",
        "\n",
        "\n",
        "replay_test_folder  =  '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/replay_data_every_10th/test'\n",
        "casia_test_folder   =  '/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/CASIA_FASD_FRAMES/test_release'\n",
        "\n",
        "#Labels\n",
        "Replay_train_full   = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/Replay_train_rgb.csv')\n",
        "Replay_dev_full     = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/Replay_dev_rgb.csv')\n",
        "Casia_train_full    = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/CASIA_FASD_TRAIN.csv')\n",
        "Casia_dev_full      =Casia_train_full.iloc[2::3]\n",
        "Casia_train_full    = Casia_train_full[Casia_train_full.index % 3 != 0]\n",
        "\n",
        "#generators\n",
        "train_gen = ImageDataGenerator(rescale=1./255,horizontal_flip=True)\n",
        "val_gen   = ImageDataGenerator(rescale=1./255,horizontal_flip=True)\n",
        "test_gen  = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# BASE MODEL - SQUEEZE-XCEPTION BOTH INTRA AND CROSS WEIGHTS\n",
        "# Fine Tuned Modifieid Exception models \n",
        "\n",
        "Replay_SE_Xception_GAP_intra          ='/content/drive/MyDrive/ML_Data/SavedModels/Xcept_GAP_Replay_9.hdf5'\n",
        "Casia_SE_Xception_GAP_intra           ='/content/drive/MyDrive/ML_Data/SavedModels/Xception_Casia_10_BN.hdf5'\n",
        "MSU_SE_Xception_GAP_intra             ='/content/drive/MyDrive/ML_Data/SavedModels/Xcept_GAP_MSU_8.hdf5'\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG7dWBm8ZnmH"
      },
      "outputs": [],
      "source": [
        "#check every =2 will check half data set, 3 will chcek one third, 4 will check 25%\n",
        "\n",
        "def TestModel(l_model,testOn,filename='result.csv',check_every=1):\n",
        "     if(testOn=='Replay'):\n",
        "      print('Loading Replay Samples')\n",
        "      test_full = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/Replay_Test_rgb.csv')\n",
        "      test_folder=replay_test_folder\n",
        "      if os.path.exists('/content/replay_data_every_10th'):\n",
        "        print('Found Local Files')\n",
        "        test_folder='/content/replay_data_every_10th/test'\n",
        "        test_full['filename'] = test_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "      if check_every >1 :\n",
        "        test_full=test_full.iloc[::check_every]\n",
        "      test_full=test_full.dropna()\n",
        "\n",
        "      testData = test_gen.flow_from_dataframe(dataframe =test_full,directory = test_folder,  x_col = \"filename\", y_col = \"label\",batch_size =16, shuffle = False, class_mode='categorical',target_size = (299,299))\n",
        "     elif(testOn=='Casia'):\n",
        "      print('Loading Casia Samples')     \n",
        "      test_full = pd.read_csv('//content/drive/MyDrive/ML_Data/Labels/CASIA_FASD_TEST.csv')\n",
        "      test_folder=casia_test_folder\n",
        "      \n",
        "      test_full=test_full.dropna()\n",
        "      if os.path.exists('/content/CASIA_FASD_FRAMES'):\n",
        "        print('Found Local Files')\n",
        "        test_folder='/content/CASIA_FASD_FRAMES/test_release'\n",
        "        test_full['filename'] = test_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "      if check_every >1 :\n",
        "        test_full=test_full.iloc[::check_every]\n",
        "      testData = test_gen.flow_from_dataframe(dataframe =test_full,directory = test_folder,  x_col = \"filename\", y_col = \"label\",batch_size =16, shuffle = False, class_mode='categorical',target_size = (299,299))\n",
        "     elif(testOn=='MSU'):    \n",
        "      print('Loading MSU Samples')\n",
        "      test_full = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/MSU_Test_Labels.csv')\n",
        "      if os.path.exists('/content/MSU-MFSD_FRAMES'):\n",
        "        print('Found Local Files')\n",
        "        test_folder='/content/MSU-MFSD_FRAMES/scene01'\n",
        "        test_full['filename'] = test_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "\n",
        "      else:\n",
        "        test_folder='/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/MSU-MFSD_FRAMES/scene01'\n",
        "\n",
        "      test_full=test_full.dropna()\n",
        "      if check_every >1 :\n",
        "        test_full=test_full.iloc[::check_every]\n",
        "      \n",
        "      testData = test_gen.flow_from_dataframe(dataframe =test_full,directory = test_folder,  x_col = \"filename\", y_col = \"label\",batch_size =16, shuffle = False, class_mode='categorical',target_size = (299,299))\n",
        "     \n",
        "     preds=l_model.predict(testData,  verbose = 1, use_multiprocessing = True, workers = 16)\n",
        "     \n",
        "     p=testData.filenames\n",
        "     lab=testData.labels\n",
        "     \n",
        "     DF = pd.DataFrame(preds)\n",
        "     DF['filenames']=p\n",
        "     DF['labels_orig']=lab\n",
        "     print('Results Done '+\"./Results/\"+filename+'.csv')\n",
        "     DF.to_csv(\"./Results/\"+filename+'.csv')\n",
        "     return DF\n",
        "def SetUp_Training(dataset='Replay',check_every=1):\n",
        "  \n",
        "  if(dataset=='Replay'):\n",
        "      print('Loading Replay Samples')\n",
        "      train_folder=Replay_train_folder\n",
        "      val_folder=Replay_val_folder\n",
        "     \n",
        "      train_full = Replay_train_full\n",
        "      dev_full = Replay_dev_full\n",
        "      if os.path.exists('/content/replay_data_every_10th'):\n",
        "         print('Found Local Files')\n",
        "         train_folder='/content/replay_data_every_10th/train'\n",
        "         val_folder='/content/replay_data_every_10th/dev'\n",
        "         train_full['filename'] = train_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "         dev_full['filename'] = dev_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "\n",
        " \n",
        "      train_1=train_full\n",
        "      dev_1=dev_full.iloc[::]\n",
        "  elif(dataset=='Casia'):\n",
        "     print('Loading Casia Samples')\n",
        "\n",
        "     train_folder=Casia_train_folder\n",
        "     val_folder=Casia_val_folder\n",
        "     dev_full = Casia_dev_full\n",
        "     train_full = Casia_train_full\n",
        "     if os.path.exists('/content/CASIA_FASD_FRAMES'): \n",
        "         print('Found Local Files')\n",
        "         train_folder='/content/CASIA_FASD_FRAMES/train_release'\n",
        "         train_full['filename'] = train_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "         dev_full['filename'] = dev_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "         val_folder=train_folder\n",
        "  # for half files\n",
        "     train_1=train_full\n",
        "     dev_1=dev_full.iloc[::]\n",
        "  elif(dataset=='MSU'):    \n",
        "      print('Loading MSU Samples')\n",
        "      train_full = pd.read_csv('/content/drive/MyDrive/ML_Data/Labels/MSU_Test_Labels.csv')# for half files\n",
        "      train_folder='/content/drive/MyDrive/ML_Data/Datasets_Preprocessed/MSU-MFSD_FRAMES'\n",
        "      if os.path.exists('/content/CASIA_FASD_FRAMES'): \n",
        "         print('Found Local Files')\n",
        "         train_folder='/content/MSU-MFSD_FRAMES'\n",
        "         train_full['filename'] = train_full['filename'].str.replace('drive/MyDrive/ML_Data/Datasets_Preprocessed/','')\n",
        "         \n",
        "      train_1=train_full[train_full.index % 4 != 0]\n",
        "      val_folder=train_folder\n",
        "      dev_1=train_full.iloc[3::4]\n",
        "  print('Training folder : '+train_folder)\n",
        "  print('Val folder : '+val_folder)\n",
        "  if check_every >1 :\n",
        "        train_1=train_1.iloc[::check_every]\n",
        "        dev_1=dev_1.iloc[::check_every]\n",
        "      \n",
        "  train_data_1 = train_gen.flow_from_dataframe(dataframe = train_1,directory = train_folder,  x_col = \"filename\", y_col = \"label\",batch_size =16, shuffle = True, class_mode='categorical',target_size = (299,299))\n",
        "  val_data_1   = val_gen.flow_from_dataframe(dataframe = dev_1, directory = val_folder, x_col = 'filename', y_col = 'label',batch_size = 16, shuffle = False, class_mode= 'categorical',target_size = (299,299))\n",
        "  return train_data_1,val_data_1     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ6ci7V8KOiz"
      },
      "outputs": [],
      "source": [
        "def getModel_ELBP_Ablation(Xweights='Replay',colors='RGB',images=False,xception_trainable=False,L1=True,L2=False,check_mode=False):\n",
        "    print('Loading xception weights')\n",
        "    \n",
        "    if(Xweights=='Replay'):\n",
        "      print('Loading xception weights -Replay')\n",
        "      loaded_model=keras.models.load_model('/content/drive/MyDrive/ML_Data/SavedModels/Xcept_GAP_Replay_9.hdf5' )\n",
        "   \n",
        "    elif(Xweights=='Casia'):\n",
        "      print('Loading xception weights - Casia')      \n",
        "      loaded_model=keras.models.load_model('/content/drive/MyDrive/ML_Data/SavedModels/Xcept_GAP_Casia_7.hdf5')\n",
        "   \n",
        "    elif(Xweights=='xcept'):\n",
        "      print('Loading xception weights - imageNet')      \n",
        "      loaded_model=SE_Xception()\n",
        "   \n",
        "    elif(Xweights=='MSU'):\n",
        "      print('Loading xception weights - MSU')      \n",
        "      loaded_model=keras.models.load_model('/content/drive/MyDrive/ML_Data/SavedModels/Xcept_GAP_MSU_10.hdf5')\n",
        "   \n",
        "\n",
        "\n",
        "    else:\n",
        "      print('Invalid Fresh Weights')\n",
        "      return\n",
        "    inputs_s = keras.Input(shape=(299, 299, 3))\n",
        "    inputs_s2 = keras.Input(shape=(299, 299, 3))\n",
        "    print('Slicing model for features')\n",
        "    X_model= Model(inputs=loaded_model.input, outputs=loaded_model.layers[-2].output,name='Squeeze_Xception')\n",
        "    Y_model= Model(inputs=loaded_model.input, outputs=loaded_model.layers[-2].output,name='Squeeze_Xception_Y')\n",
        "    x_out=X_model(inputs_s2,training=False)\n",
        "      \n",
        "      #common layer to get face data . this will pass the colored cropped face to ELBP and HSV generator\n",
        "    faceGetter=FacePlate(input_dim=(299,299,1))(inputs_s2)\n",
        "    iLBp=keras.Input(shape=(faceGetter.shape[1],faceGetter.shape[2],faceGetter.shape[3]))\n",
        "    R_=tf.expand_dims(iLBp[:,:,:,0],-1)\n",
        "    G_=tf.expand_dims(iLBp[:,:,:,1],-1)\n",
        "    B_=tf.expand_dims(iLBp[:,:,:,2],-1)\n",
        "    R_i=tf.expand_dims(inputs_s2[:,:,:,0],-1)\n",
        "    G_i=tf.expand_dims(inputs_s2[:,:,:,1],-1)\n",
        "    B_i=tf.expand_dims(inputs_s2[:,:,:,2],-1)\n",
        "    HSV=tf.image.rgb_to_hsv(iLBp)\n",
        "    V_=tf.expand_dims(HSV[:,:,:,2],-1)    \n",
        "    S_=tf.expand_dims(HSV[:,:,:,1],-1)    \n",
        "    H_=tf.expand_dims(HSV[:,:,:,0],-1)    \n",
        "\n",
        "    #Face\n",
        "    Y_=tf.keras.layers.Add()([Lambda(lambda x: x * 0.299)(R_),Lambda(lambda x: x * 0.587)(G_),Lambda(lambda x: x * 0.114)(B_)])\n",
        "    Cb_=tf.keras.layers.Add()([Lambda(lambda x: x * (-0.168736))(R_),Lambda(lambda x: x * (-0.331264))(G_),Lambda(lambda x: x * 0.5)(B_)])\n",
        "    Cr_=tf.keras.layers.Add()([Lambda(lambda x: x *0.5)(R_),Lambda(lambda x: x * (-0.418688))(G_),Lambda(lambda x: x * (-0.081312))(B_)])\n",
        "    Y_i=tf.keras.layers.Add()([Lambda(lambda x: x * 0.299)(R_i),Lambda(lambda x: x * 0.587)(G_i),Lambda(lambda x: x * 0.114)(B_i)])\n",
        "    Cb_i=tf.keras.layers.Add()([Lambda(lambda x: x * (-0.168736))(R_i),Lambda(lambda x: x * (-0.331264))(G_i),Lambda(lambda x: x * 0.5)(B_i)])\n",
        "    Cr_i=tf.keras.layers.Add()([Lambda(lambda x: x *0.5)(R_i),Lambda(lambda x: x * (-0.418688))(G_i),Lambda(lambda x: x * (-0.081312))(B_i)])\n",
        "    YCbCr=keras.layers.concatenate([Y_i,Cb_i,Cr_i],name='ycbcb')\n",
        "    rgY=keras.Model(inputs_s2,YCbCr,name='RGB_To_YCbCr')\n",
        "    print(YCbCr)\n",
        "    y_out=Y_model(rgY(inputs_s2))\n",
        "    print(y_out)\n",
        "    \n",
        "    R_ELB_out=  Slice_ELBP_Model  (iLBp,R_ ,n='R_'  ,l1=L1,l2=L2)(faceGetter)\n",
        "    G_ELB_out=  Slice_ELBP_Model  (iLBp,G_ ,n='G_'  ,l1=L1,l2=L2)(faceGetter)\n",
        "    B_ELB_out=  Slice_ELBP_Model  (iLBp,B_ ,n='B_'  ,l1=L1,l2=L2)(faceGetter)\n",
        "    Y_ELB_out=  Slice_ELBP_Model  (iLBp,Y_ ,n='Y_'  ,l1=L1,l2=L2)(faceGetter)\n",
        "    Cb_ELB_out= Slice_ELBP_Model (iLBp,Cb_,n='Cb_',l1=L1 ,l2=L2) (faceGetter)\n",
        "    Cr_ELB_out= Slice_ELBP_Model (iLBp,Cr_,n='Cr_',l1=L1 ,l2=L2) (faceGetter)\n",
        "    V_ELB_out = Slice_ELBP_Model(iLBp,V_ ,n='V_' ,l1=L1 ,l2=L2) (faceGetter)\n",
        "    H_ELB_out = Slice_ELBP_Model(iLBp,H_ ,n='H_' ,l1=L1 ,l2=L2) (faceGetter)\n",
        "    S_ELB_out = Slice_ELBP_Model(iLBp,S_ ,n='S_' ,l1=L1 ,l2=L2)(faceGetter)\n",
        "    # connect all\n",
        "    if colors=='RGB':\n",
        "      el_c=keras.layers.concatenate([R_ELB_out,G_ELB_out,B_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='HSV':\n",
        "      el_c=keras.layers.concatenate([H_ELB_out,S_ELB_out,V_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='YCbCr':\n",
        "      el_c=keras.layers.concatenate([Y_ELB_out,Cr_ELB_out,Cb_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='YCbCr+HSV':\n",
        "      el_c=keras.layers.concatenate([Y_ELB_out,Cr_ELB_out,Cb_ELB_out,H_ELB_out,S_ELB_out,V_ELB_out],axis=1,name='lbp_concat_all')  \n",
        "    elif colors=='RGB+HSV':\n",
        "      el_c=keras.layers.concatenate([R_ELB_out,G_ELB_out,B_ELB_out,H_ELB_out,S_ELB_out,V_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='RGB+V':\n",
        "      el_c=keras.layers.concatenate([R_ELB_out,G_ELB_out,B_ELB_out,V_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='RGB+YCbCr':\n",
        "      el_c=keras.layers.concatenate([R_ELB_out,G_ELB_out,B_ELB_out,Y_ELB_out,Cr_ELB_out,Cb_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    elif colors=='RGB+YCbCr+V':\n",
        "      el_c=keras.layers.concatenate([R_ELB_out,G_ELB_out,B_ELB_out,V_ELB_out,Y_ELB_out,Cr_ELB_out,Cb_ELB_out],axis=1,name='lbp_concat_all')\n",
        "    else:\n",
        "      print('Unknown color combo')\n",
        "      return 0\n",
        "    \n",
        "   \n",
        "    el_c=LayerNormalization()(el_c)\n",
        "    el_c=keras.layers.Dense(512,activation='relu',name='e_dense_1208')(el_c)\n",
        "    el_c=keras.layers.Dense(32,activation='relu',name='e_dense_128')(el_c)\n",
        "    \n",
        "    \n",
        "    el_c=keras.layers.concatenate([x_out,el_c])\n",
        "    el_c=keras.layers.Dense(64,activation='relu',name='f_dense_128')(el_c)\n",
        "\n",
        "    \n",
        "    \n",
        "   \n",
        "\n",
        "  \n",
        "    outputs = keras.layers.Dense(2,activation='softmax',name='e_dense_3')(el_c)\n",
        "    f_model=keras.Model(inputs_s2,outputs)\n",
        "\n",
        "\n",
        "    \n",
        "    f_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=\"categorical_accuracy\",steps_per_execution=8)\n",
        "    if xception_trainable:\n",
        "      print('xception sub model is trainable!')\n",
        "    else  :\n",
        "      print('xception sub model is NOT trainable!')\n",
        "    f_model.get_layer('Squeeze_Xception').trainable=xception_trainable\n",
        "    #f_model.get_layer('Squeeze_Xception_Y').trainable=xception_trainable\n",
        "    if(images):\n",
        "      model_photu(LBP_container,name='Full_model')\n",
        "      print('Model image saved to : '+os.getcwd()+'/model_images/Full_model.png')\n",
        "    return f_model    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inj-b8ai8i2y"
      },
      "outputs": [],
      "source": [
        "f_model=getModel_ELBP_Ablation(Xweights='Casia',colors='RGB+YCbCr+V',L1=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k048olUHFq4W"
      },
      "outputs": [],
      "source": [
        "#Training - Casia\n",
        "\n",
        "train_data_1,val_data_1=SetUp_Training(dataset='Casia',check_every=1)\n",
        "\n",
        "f_model.fit(train_data_1,epochs=20, validation_data=val_data_1, verbose = 1,use_multiprocessing=True,workers=16)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mitHyuzK5T4n"
      },
      "outputs": [],
      "source": [
        "#Testing \n",
        "\n",
        "TestModel(f_model,'Replay','R_on_C_RGB_Wide',check_every=5)\n",
        "print(Parameters_name_list(['R_on_C_RGB_Wide']))\n",
        "TestModel(f_model,'Casia','C_on_C_RGB_Wide',check_every=5)\n",
        "print(Parameters_name_list(['C_on_C_RGB_Wide']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TINJRcJi5h56"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data_1,val_data_1=SetUp_Training(dataset='Replay',check_every=1)\n",
        "\n",
        "f_model.fit(train_data_1,epochs=20, validation_data=val_data_1, verbose = 1,use_multiprocessing=True,workers=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFFnrqxJ53vc"
      },
      "outputs": [],
      "source": [
        "TestModel(f_model,'Replay','R_on_R_All_Wide',check_every=5)\n",
        "print(Parameters_name_list(['R_on_R_All_Wide']))\n",
        "TestModel(f_model,'Casia','C_on_R_All_Wide',check_every=5)\n",
        "print(Parameters_name_list(['C_on_R_All_Wide']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfRxGxVjSidw"
      },
      "outputs": [],
      "source": [
        "TestModel(f_model,'Replay','R_on_C_RGB_YCbCr_HSV',check_every=4)\n",
        "print(Parameters_name_list(['R_on_C_RGB_YCbCr_HSV']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPbO52dOzzlS"
      },
      "outputs": [],
      "source": [
        "TestModel(f_model,'Replay','R_on_R_RGB_YCbCr_HSV',check_every=2)\n",
        "print(Parameters_name_list(['R_on_R_RGB_YCbCr_HSV']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vodYPSb6PLXL"
      },
      "outputs": [],
      "source": [
        "df=Parameters_name_list(['R_on_C_RGB_YCbCr_HSV','C_on_C_RGB_YCbCr_L012_2','M_on_C_RGB_YCbCr_L012',\n",
        "                            'R_on_R_RGB_YCbCr_HSV','M_on_R_YCbCr_L012','C_on_R_RGB_YCbCr_V_L012_2',\n",
        "                            'C_on_M_RGB_YCbCrV_L012_512','R_on_M_L012H_512_1','M_on_M_L012H_512'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjyCnJwzPRc4"
      },
      "outputs": [],
      "source": [
        "df.to_csv('/content/final_Results.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqnufbY4PWdR"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1-YslmKeGQJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}